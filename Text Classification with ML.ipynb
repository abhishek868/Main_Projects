{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas  as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB,BernoulliNB\n",
    "nb = MultinomialNB()\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder \n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.svm import SVC\n",
    "from scipy.sparse import hstack, vstack\n",
    "from nltk.corpus import stopwords  \n",
    "#nltk.download('stopwords') \n",
    "import pickle\n",
    "import datetime\n",
    "from dateutil import parser\n",
    "import sys\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import f1_score ,accuracy_score,confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#english_stop=[\"a\",\"about\",\"above\",\"after\",\"again\",\"against\",\"all\",\"am\",\"an\",\"and\",\"any\",\"are\",\"aren't\",\"as\",\"at\",\"be\",\"because\",\"been\",\"before\",\"being\",\"below\",\"between\",\"both\",\"but\",\"by\",\"during\",\"each\",\"few\",\"for\",\"from\",\"further\",\"having\",\"he\",\"he'd\",\"he'll\",\"he's\",\"her\",\"here\",\"here's\",\"hers\",\"herself\",\"him\",\"himself\",\"his\",\"how\",\"how's\",\"i\",\"i'd\",\"i'll\",\"i'm\",\"i've\",\"if\",\"in\",\"into\",\"is\",\"isn't\",\"it\",\"it's\",\"its\",\"itself\",\"let's\",\"me\",\"more\",\"most\",\"mustn't\",\"my\",\"myself\",\"no\",\"nor\",\"not\",\"of\",\"off\",\"on\",\"once\",\"only\",\"or\",\"other\",\"ought\",\"our\",\"ours\",\"ourselves\",\"out\",\"over\",\"own\",\"same\",\"shan't\",\"she\",\"she'd\",\"she'll\",\"she's\",\"should\",\"shouldn't\",\"so\",\"some\",\"such\",\"than\",\"that\",\"that's\",\"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"there\",\"there's\",\"these\",\"they\",\"they'd\",\"they'll\",\"they're\",\"they've\",\"this\",\"those\",\"through\",\"to\",\"too\",\"under\",\"until\",\"up\",\"very\",\"was\",\"we\",\"we'd\",\"we'll\",\"we're\",\"we've\",\"were\",\"weren't\",\"what\",\"what's\",\"when\",\"when's\",\"where\",\"where's\",\"which\",\"while\",\"who\",\"who's\",\"whom\",\"why\",\"why's\",\"with\",\"you\",\"you'd\",\"you'll\",\"you're\",\"you've\",\"your\",\"yours\",\"yourself\",\"yourselves\"]\n",
    "#self_stop=[\"end\", \"dell\",\"del\", \"support\",\"chromebook\", \"chrome book\", \"bad\",\"good\", \"doesnt\", \"isnt\", \"latitude\", \"power edge\", \"power edge\", \"working\", \"issue\", \"SR\", \"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\", \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\", \"xx\", \"xxx\", \"-\", \"_\", \".\", \",\", \"iw\", \"micro\"]\n",
    "#stopwords = english_stop+ self_stop\n",
    "#stopwords = list(map(lambda x: x.upper(), english_stop))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#re.sub(r'[^\\w]', ' ', data[\"NOTE_TXT\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>FWEEK</th>\n",
       "      <th>FMONTH</th>\n",
       "      <th>FQUARTER</th>\n",
       "      <th>FYEAR</th>\n",
       "      <th>DPS_NUM</th>\n",
       "      <th>PRODUCT_DESC</th>\n",
       "      <th>BRAND_DESC</th>\n",
       "      <th>DPSTYPE</th>\n",
       "      <th>DPSDATE</th>\n",
       "      <th>...</th>\n",
       "      <th>Transactional_TSL</th>\n",
       "      <th>Dispatch cancellation time</th>\n",
       "      <th>Day Difference b/w DPS creation &amp; cancellation</th>\n",
       "      <th>Can cancellation reason be determined using Comments from Vendor</th>\n",
       "      <th>CANCELLED_BY_AGENT</th>\n",
       "      <th>CANCELLED_BY_MGRNAME</th>\n",
       "      <th>CANCELLED_BY_AMGRNAME</th>\n",
       "      <th>Language</th>\n",
       "      <th>Reason_for_Cancellation_COE</th>\n",
       "      <th>NOTES_COE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>201935</td>\n",
       "      <td>2019M09</td>\n",
       "      <td>2019Q03</td>\n",
       "      <td>2019</td>\n",
       "      <td>370999887</td>\n",
       "      <td>OptiPlex Desktops</td>\n",
       "      <td>OPTIPLEX 7060</td>\n",
       "      <td>NBU</td>\n",
       "      <td>2018-10-04 00:27:28.0000000</td>\n",
       "      <td>...</td>\n",
       "      <td>Kelly Truax</td>\n",
       "      <td>2018-10-13 21:47:38.000000</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Ogan, Jason</td>\n",
       "      <td>Smith, Frederick</td>\n",
       "      <td>Haworth, Scott</td>\n",
       "      <td>English</td>\n",
       "      <td>TS DUPLICATE DPS CREATED</td>\n",
       "      <td>BY DISPATCH NO LONGER NEED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>201936</td>\n",
       "      <td>2019M09</td>\n",
       "      <td>2019Q03</td>\n",
       "      <td>2019</td>\n",
       "      <td>372031241</td>\n",
       "      <td>Chrome</td>\n",
       "      <td>CHROMEBOOK 11 3180</td>\n",
       "      <td>RTS</td>\n",
       "      <td>2018-10-12 15:29:56.0000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-11-06 15:48:02.000000</td>\n",
       "      <td>25.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Suderman, Kevin</td>\n",
       "      <td>Fite, David</td>\n",
       "      <td>Haworth, Scott</td>\n",
       "      <td>English</td>\n",
       "      <td>TS DUPLICATE DPS CREATED</td>\n",
       "      <td>BY WA ALREADI SHIP BACK CUSTOM SHOW A PICTUR O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>201937</td>\n",
       "      <td>2019M09</td>\n",
       "      <td>2019Q03</td>\n",
       "      <td>2019</td>\n",
       "      <td>17372358148</td>\n",
       "      <td>OptiPlex Desktops</td>\n",
       "      <td>OPTIPLEX 3040</td>\n",
       "      <td>NBD</td>\n",
       "      <td>2018-10-16 13:53:11.0000000</td>\n",
       "      <td>...</td>\n",
       "      <td>Kelly Truax</td>\n",
       "      <td>2018-10-19 14:22:33.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Bakowski, Mateusz</td>\n",
       "      <td>FakeLast, FakeFirst</td>\n",
       "      <td>FakeLast, FakeFirst</td>\n",
       "      <td>English</td>\n",
       "      <td>TS DUPLICATE DPS CREATED</td>\n",
       "      <td>SEE COMMENT WRONG EMAIL LOG ISSU NO POWER HARD...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>201935</td>\n",
       "      <td>2019M09</td>\n",
       "      <td>2019Q03</td>\n",
       "      <td>2019</td>\n",
       "      <td>385610825</td>\n",
       "      <td>Latitude</td>\n",
       "      <td>LATITUDE 5550</td>\n",
       "      <td>RTS</td>\n",
       "      <td>2018-10-03 19:33:09.0000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-10-04 16:12:46.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Kiper, Kyle</td>\n",
       "      <td>Windish, Daniel</td>\n",
       "      <td>Hernandez, Victor</td>\n",
       "      <td>English</td>\n",
       "      <td>TS DUPLICATE DPS CREATED</td>\n",
       "      <td>ALREADI RECREAT THI ONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>201935</td>\n",
       "      <td>2019M09</td>\n",
       "      <td>2019Q03</td>\n",
       "      <td>2019</td>\n",
       "      <td>370861455</td>\n",
       "      <td>Alienware Notebooks</td>\n",
       "      <td>ALIENWARE 17 R3</td>\n",
       "      <td>RPQ</td>\n",
       "      <td>2018-10-03 02:02:01.0000000</td>\n",
       "      <td>...</td>\n",
       "      <td>Denis Kelly</td>\n",
       "      <td>2018-10-10 02:42:44.000000</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>WalkToManifest, WalkToManifest</td>\n",
       "      <td>Manager, TBD</td>\n",
       "      <td>Manager, TBD</td>\n",
       "      <td>English</td>\n",
       "      <td>TS DUPLICATE DPS CREATED</td>\n",
       "      <td>BY QLX CANCEL BY DSP DSP WORK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0   FWEEK   FMONTH FQUARTER  FYEAR      DPS_NUM  \\\n",
       "0           0  201935  2019M09  2019Q03   2019    370999887   \n",
       "1           1  201936  2019M09  2019Q03   2019    372031241   \n",
       "2           2  201937  2019M09  2019Q03   2019  17372358148   \n",
       "3           3  201935  2019M09  2019Q03   2019    385610825   \n",
       "4           4  201935  2019M09  2019Q03   2019    370861455   \n",
       "\n",
       "          PRODUCT_DESC          BRAND_DESC DPSTYPE  \\\n",
       "0    OptiPlex Desktops       OPTIPLEX 7060     NBU   \n",
       "1               Chrome  CHROMEBOOK 11 3180     RTS   \n",
       "2    OptiPlex Desktops       OPTIPLEX 3040     NBD   \n",
       "3             Latitude       LATITUDE 5550     RTS   \n",
       "4  Alienware Notebooks     ALIENWARE 17 R3     RPQ   \n",
       "\n",
       "                       DPSDATE  ...  Transactional_TSL  \\\n",
       "0  2018-10-04 00:27:28.0000000  ...        Kelly Truax   \n",
       "1  2018-10-12 15:29:56.0000000  ...                NaN   \n",
       "2  2018-10-16 13:53:11.0000000  ...        Kelly Truax   \n",
       "3  2018-10-03 19:33:09.0000000  ...                NaN   \n",
       "4  2018-10-03 02:02:01.0000000  ...        Denis Kelly   \n",
       "\n",
       "   Dispatch cancellation time Day Difference b/w DPS creation & cancellation  \\\n",
       "0  2018-10-13 21:47:38.000000                                            9.0   \n",
       "1  2018-11-06 15:48:02.000000                                           25.0   \n",
       "2  2018-10-19 14:22:33.000000                                            3.0   \n",
       "3  2018-10-04 16:12:46.000000                                            1.0   \n",
       "4  2018-10-10 02:42:44.000000                                            7.0   \n",
       "\n",
       "  Can cancellation reason be determined using Comments from Vendor  \\\n",
       "0                                                Yes                 \n",
       "1                                                Yes                 \n",
       "2                                                Yes                 \n",
       "3                                                Yes                 \n",
       "4                                                Yes                 \n",
       "\n",
       "               CANCELLED_BY_AGENT CANCELLED_BY_MGRNAME CANCELLED_BY_AMGRNAME  \\\n",
       "0                     Ogan, Jason     Smith, Frederick        Haworth, Scott   \n",
       "1                 Suderman, Kevin          Fite, David        Haworth, Scott   \n",
       "2               Bakowski, Mateusz  FakeLast, FakeFirst   FakeLast, FakeFirst   \n",
       "3                     Kiper, Kyle      Windish, Daniel     Hernandez, Victor   \n",
       "4  WalkToManifest, WalkToManifest         Manager, TBD          Manager, TBD   \n",
       "\n",
       "  Language  Reason_for_Cancellation_COE  \\\n",
       "0  English     TS DUPLICATE DPS CREATED   \n",
       "1  English     TS DUPLICATE DPS CREATED   \n",
       "2  English     TS DUPLICATE DPS CREATED   \n",
       "3  English     TS DUPLICATE DPS CREATED   \n",
       "4  English     TS DUPLICATE DPS CREATED   \n",
       "\n",
       "                                           NOTES_COE  \n",
       "0                         BY DISPATCH NO LONGER NEED  \n",
       "1  BY WA ALREADI SHIP BACK CUSTOM SHOW A PICTUR O...  \n",
       "2  SEE COMMENT WRONG EMAIL LOG ISSU NO POWER HARD...  \n",
       "3                            ALREADI RECREAT THI ONE  \n",
       "4                      BY QLX CANCEL BY DSP DSP WORK  \n",
       "\n",
       "[5 rows x 54 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_excel(\"C:\\\\Users\\\\Abhishek_Prasar\\\\Documents\\\\My Received Files\\\\train_29_march_preprocessed.xlsx\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "colon = '\\uff1a'\n",
    "semicolon = '\\uff1b'\n",
    "comma = '\\uff0c'\n",
    "enum_comma = '\\u3001'\n",
    "left_parenthesis = '\\uff08'\n",
    "right_parenthesis = '\\uff09'\n",
    "period = '\\uff0e'\n",
    "japanese_square_bullet = '\\u25a0'\n",
    "japanese_diamond_bullet = '\\u25c6'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.NOTE_TXT = list(map(lambda x: x.replace(comma,\",\").replace(enum_comma, \",\").replace(period, \".\").replace(semicolon,\";\").replace(colon,\":\").replace(left_parenthesis,\"(\" ).replace(right_parenthesis,\")\"), data.NOTE_TXT.astype(str)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>FWEEK</th>\n",
       "      <th>FMONTH</th>\n",
       "      <th>FQUARTER</th>\n",
       "      <th>FYEAR</th>\n",
       "      <th>DPS_NUM</th>\n",
       "      <th>PRODUCT_DESC</th>\n",
       "      <th>BRAND_DESC</th>\n",
       "      <th>DPSTYPE</th>\n",
       "      <th>DPSDATE</th>\n",
       "      <th>...</th>\n",
       "      <th>Dispatch cancellation time</th>\n",
       "      <th>Day Difference b/w DPS creation &amp; cancellation</th>\n",
       "      <th>Can cancellation reason be determined using Comments from Vendor</th>\n",
       "      <th>CANCELLED_BY_AGENT</th>\n",
       "      <th>CANCELLED_BY_MGRNAME</th>\n",
       "      <th>CANCELLED_BY_AMGRNAME</th>\n",
       "      <th>Language</th>\n",
       "      <th>Reason_for_Cancellation_COE</th>\n",
       "      <th>NOTES_COE</th>\n",
       "      <th>Final_Category_Encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>201935</td>\n",
       "      <td>2019M09</td>\n",
       "      <td>2019Q03</td>\n",
       "      <td>2019</td>\n",
       "      <td>370999887</td>\n",
       "      <td>OptiPlex Desktops</td>\n",
       "      <td>OPTIPLEX 7060</td>\n",
       "      <td>NBU</td>\n",
       "      <td>2018-10-04 00:27:28.0000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2018-10-13 21:47:38.000000</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Ogan, Jason</td>\n",
       "      <td>Smith, Frederick</td>\n",
       "      <td>Haworth, Scott</td>\n",
       "      <td>English</td>\n",
       "      <td>TS DUPLICATE DPS CREATED</td>\n",
       "      <td>BY DISPATCH NO LONGER NEED</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>201936</td>\n",
       "      <td>2019M09</td>\n",
       "      <td>2019Q03</td>\n",
       "      <td>2019</td>\n",
       "      <td>372031241</td>\n",
       "      <td>Chrome</td>\n",
       "      <td>CHROMEBOOK 11 3180</td>\n",
       "      <td>RTS</td>\n",
       "      <td>2018-10-12 15:29:56.0000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2018-11-06 15:48:02.000000</td>\n",
       "      <td>25.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Suderman, Kevin</td>\n",
       "      <td>Fite, David</td>\n",
       "      <td>Haworth, Scott</td>\n",
       "      <td>English</td>\n",
       "      <td>TS DUPLICATE DPS CREATED</td>\n",
       "      <td>BY WA ALREADI SHIP BACK CUSTOM SHOW A PICTUR O...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>201937</td>\n",
       "      <td>2019M09</td>\n",
       "      <td>2019Q03</td>\n",
       "      <td>2019</td>\n",
       "      <td>17372358148</td>\n",
       "      <td>OptiPlex Desktops</td>\n",
       "      <td>OPTIPLEX 3040</td>\n",
       "      <td>NBD</td>\n",
       "      <td>2018-10-16 13:53:11.0000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2018-10-19 14:22:33.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Bakowski, Mateusz</td>\n",
       "      <td>FakeLast, FakeFirst</td>\n",
       "      <td>FakeLast, FakeFirst</td>\n",
       "      <td>English</td>\n",
       "      <td>TS DUPLICATE DPS CREATED</td>\n",
       "      <td>SEE COMMENT WRONG EMAIL LOG ISSU NO POWER HARD...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>201935</td>\n",
       "      <td>2019M09</td>\n",
       "      <td>2019Q03</td>\n",
       "      <td>2019</td>\n",
       "      <td>385610825</td>\n",
       "      <td>Latitude</td>\n",
       "      <td>LATITUDE 5550</td>\n",
       "      <td>RTS</td>\n",
       "      <td>2018-10-03 19:33:09.0000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2018-10-04 16:12:46.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Kiper, Kyle</td>\n",
       "      <td>Windish, Daniel</td>\n",
       "      <td>Hernandez, Victor</td>\n",
       "      <td>English</td>\n",
       "      <td>TS DUPLICATE DPS CREATED</td>\n",
       "      <td>ALREADI RECREAT THI ONE</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>201935</td>\n",
       "      <td>2019M09</td>\n",
       "      <td>2019Q03</td>\n",
       "      <td>2019</td>\n",
       "      <td>370861455</td>\n",
       "      <td>Alienware Notebooks</td>\n",
       "      <td>ALIENWARE 17 R3</td>\n",
       "      <td>RPQ</td>\n",
       "      <td>2018-10-03 02:02:01.0000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2018-10-10 02:42:44.000000</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>WalkToManifest, WalkToManifest</td>\n",
       "      <td>Manager, TBD</td>\n",
       "      <td>Manager, TBD</td>\n",
       "      <td>English</td>\n",
       "      <td>TS DUPLICATE DPS CREATED</td>\n",
       "      <td>BY QLX CANCEL BY DSP DSP WORK</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0   FWEEK   FMONTH FQUARTER  FYEAR      DPS_NUM  \\\n",
       "0           0  201935  2019M09  2019Q03   2019    370999887   \n",
       "1           1  201936  2019M09  2019Q03   2019    372031241   \n",
       "2           2  201937  2019M09  2019Q03   2019  17372358148   \n",
       "3           3  201935  2019M09  2019Q03   2019    385610825   \n",
       "4           4  201935  2019M09  2019Q03   2019    370861455   \n",
       "\n",
       "          PRODUCT_DESC          BRAND_DESC DPSTYPE  \\\n",
       "0    OptiPlex Desktops       OPTIPLEX 7060     NBU   \n",
       "1               Chrome  CHROMEBOOK 11 3180     RTS   \n",
       "2    OptiPlex Desktops       OPTIPLEX 3040     NBD   \n",
       "3             Latitude       LATITUDE 5550     RTS   \n",
       "4  Alienware Notebooks     ALIENWARE 17 R3     RPQ   \n",
       "\n",
       "                       DPSDATE  ...  Dispatch cancellation time  \\\n",
       "0  2018-10-04 00:27:28.0000000  ...  2018-10-13 21:47:38.000000   \n",
       "1  2018-10-12 15:29:56.0000000  ...  2018-11-06 15:48:02.000000   \n",
       "2  2018-10-16 13:53:11.0000000  ...  2018-10-19 14:22:33.000000   \n",
       "3  2018-10-03 19:33:09.0000000  ...  2018-10-04 16:12:46.000000   \n",
       "4  2018-10-03 02:02:01.0000000  ...  2018-10-10 02:42:44.000000   \n",
       "\n",
       "  Day Difference b/w DPS creation & cancellation  \\\n",
       "0                                            9.0   \n",
       "1                                           25.0   \n",
       "2                                            3.0   \n",
       "3                                            1.0   \n",
       "4                                            7.0   \n",
       "\n",
       "  Can cancellation reason be determined using Comments from Vendor  \\\n",
       "0                                                Yes                 \n",
       "1                                                Yes                 \n",
       "2                                                Yes                 \n",
       "3                                                Yes                 \n",
       "4                                                Yes                 \n",
       "\n",
       "               CANCELLED_BY_AGENT CANCELLED_BY_MGRNAME CANCELLED_BY_AMGRNAME  \\\n",
       "0                     Ogan, Jason     Smith, Frederick        Haworth, Scott   \n",
       "1                 Suderman, Kevin          Fite, David        Haworth, Scott   \n",
       "2               Bakowski, Mateusz  FakeLast, FakeFirst   FakeLast, FakeFirst   \n",
       "3                     Kiper, Kyle      Windish, Daniel     Hernandez, Victor   \n",
       "4  WalkToManifest, WalkToManifest         Manager, TBD          Manager, TBD   \n",
       "\n",
       "  Language Reason_for_Cancellation_COE  \\\n",
       "0  English    TS DUPLICATE DPS CREATED   \n",
       "1  English    TS DUPLICATE DPS CREATED   \n",
       "2  English    TS DUPLICATE DPS CREATED   \n",
       "3  English    TS DUPLICATE DPS CREATED   \n",
       "4  English    TS DUPLICATE DPS CREATED   \n",
       "\n",
       "                                           NOTES_COE  Final_Category_Encoded  \n",
       "0                         BY DISPATCH NO LONGER NEED                      18  \n",
       "1  BY WA ALREADI SHIP BACK CUSTOM SHOW A PICTUR O...                      18  \n",
       "2  SEE COMMENT WRONG EMAIL LOG ISSU NO POWER HARD...                      18  \n",
       "3                            ALREADI RECREAT THI ONE                      18  \n",
       "4                      BY QLX CANCEL BY DSP DSP WORK                      18  \n",
       "\n",
       "[5 rows x 55 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "lb = LabelEncoder()\n",
    "data['Final_Category_Encoded'] = lb.fit_transform(data['Reason_for_Cancellation_COE'])\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"NOTES_COE\"] = data[\"NOTES_COE\"].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lb_name_mapping = dict(zip(lb.classes_, lb.transform(lb.classes_)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code\n",
    "\"\"\"\n",
    "def first_k(s: str, k=200) -> str:\n",
    "    s = str(s)  # just in case something like NaN tries to sneak in there\n",
    "    first_words = s.split()[:k]\n",
    "    return ' '.join(first_words)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#data[\"NOTE_TXT\"] = data[\"NOTE_TXT\"].apply(first_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = data.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"NOTES_COE\"] = data[\"NOTES_COE\"].astype(str)\n",
    "data[\"NOTES_COE\"] = [x.replace(':',' ') for x in data[\"NOTES_COE\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing Punctuations\n",
    "data[\"NOTES_COE\"]  = data[\"NOTES_COE\"] .str.replace('[^\\w\\s]','')\n",
    "data[\"NOTES_COE\"] .head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "data[\"NOTES_COE\"] = data[\"NOTES_COE\"].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "data[\"NOTES_COE\"].head()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#Rare Word Removal\n",
    "freq = pd.Series(' '.join(data[\"NOTES_COE\"]).split()).value_counts()[-20:]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "freq = list(freq.index)\n",
    "data[\"NOTES_COE\"] = data[\"NOTES_COE\"].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "data[\"NOTES_COE\"].head()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spelling correction\n",
    "#from textblob import TextBlob\n",
    "#data[\"NOTES_COE\"].apply(lambda x: str(TextBlob(x).correct()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizing\n",
    "#from textblob import TextBlob\n",
    "#TextBlob(data[\"NOTES_COE\"].astype(str)).words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "st = PorterStemmer()\n",
    "data[\"NOTES_COE\"].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Lemmatization\n",
    "from textblob import Word\n",
    "data[\"NOTES_COE\"] = data[\"NOTES_COE\"].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "data[\"NOTES_COE\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "word_checker = pd.read_excel(\"C:\\\\Users\\\\Abhishek_Prasar\\\\Desktop\\\\data\\\\Shreejit.xlsx\",sheet_name=\"Sheet1\")\n",
    "\n",
    "word_checker[\"NOTE_TXT\"] = word_checker[\"NOTE_TXT\"].str.upper()\n",
    "word_checker[\"NOTE_TXT\"] = [x.replace(':', '  ') for x in word_checker[\"NOTE_TXT\"]]\n",
    "#Removing Punctuations\n",
    "word_checker[\"NOTE_TXT\"] = word_checker[\"NOTE_TXT\"].str.replace('[^\\w\\s]','')\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "word_checker[\"NOTE_TXT\"] = word_checker[\"NOTE_TXT\"].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "#Rare Word Removal\n",
    "freq = pd.Series(' '.join(word_checker[\"NOTE_TXT\"]).split()).value_counts()[-20:]\n",
    "#Rare Word Removal\n",
    "freq = pd.Series(' '.join(word_checker[\"NOTE_TXT\"]).split()).value_counts()[-20:]\n",
    "\n",
    "freq = list(freq.index)\n",
    "word_checker[\"NOTE_TXT\"] = word_checker[\"NOTE_TXT\"].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "word_checker[\"NOTE_TXT\"].head()\n",
    "\n",
    "#Lemmatization\n",
    "from textblob import Word\n",
    "word_checker[\"NOTE_TXT\"] = word_checker[\"NOTE_TXT\"].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "word_checker[\"NOTE_TXT\"].head()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unique Find Finder\n",
    "\"\"\"\n",
    "import re\n",
    " \n",
    "\n",
    "regex = re.compile(r\"[\\W_]\")\n",
    "unique_words = set()\n",
    "for line in word_checker[\"NOTE_TXT\"]:\n",
    "    line = line.rstrip('\\n')\n",
    "    words = regex.split(line)\n",
    "    for w in words:\n",
    "        if w != \"\":\n",
    "            w = w.upper()\n",
    "            unique_words.add(w)\n",
    " \n",
    "print(unique_words)\n",
    "unique_words = list(unique_words)\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data[\"NOTES_COE\"] = data[\"NOTES_COE\"].apply(lambda x: \" \".join(x for x in x.split() if x in unique_words))\n",
    "#data[\"NOTES_COE\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.to_excel(\"D://my_data//for_keras.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COE_NOTES</th>\n",
       "      <th>Reason_for_Cancellation_ENT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cancellation requested by: QLX. Reason: CANCEL...</td>\n",
       "      <td>CUSTOMER NOT AVAILABLE/REACHABLE/ SYSTEM NOT A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cancellation requested by: Dell. Reason:Aging ...</td>\n",
       "      <td>CUSTOMER DENIED SERVICE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cancellation requested by: QLX. Reason: CANCEL...</td>\n",
       "      <td>LOGISTICS PNA/CNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cancellation requested by: Dell. Reason:Incide...</td>\n",
       "      <td>CUSTOMER REQUESTED DIFFERENT DATE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cancellation requested by: QLX. Reason: CANCEL...</td>\n",
       "      <td>CUSTOMER REQUESTED DIFFERENT DATE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           COE_NOTES  \\\n",
       "0  Cancellation requested by: QLX. Reason: CANCEL...   \n",
       "1  Cancellation requested by: Dell. Reason:Aging ...   \n",
       "2  Cancellation requested by: QLX. Reason: CANCEL...   \n",
       "3  Cancellation requested by: Dell. Reason:Incide...   \n",
       "4  Cancellation requested by: QLX. Reason: CANCEL...   \n",
       "\n",
       "                         Reason_for_Cancellation_ENT  \n",
       "0  CUSTOMER NOT AVAILABLE/REACHABLE/ SYSTEM NOT A...  \n",
       "1                            CUSTOMER DENIED SERVICE  \n",
       "2                                  LOGISTICS PNA/CNS  \n",
       "3                  CUSTOMER REQUESTED DIFFERENT DATE  \n",
       "4                  CUSTOMER REQUESTED DIFFERENT DATE  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid = pd.read_excel(\"C:\\\\Users\\\\Abhishek_Prasar\\\\Documents\\\\My Received Files\\\\test_29_march_preprocessed.xlsx\")\n",
    "valid[['COE_NOTES','Reason_for_Cancellation_ENT']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid['Final_Category_Encoded1'] = lb.transform(valid['Reason_for_Cancellation_ENT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "valid[\"COE_NOTES\"] = valid[\"COE_NOTES\"].str.upper()\n",
    "valid[\"COE_NOTES\"] = [x.replace(':', '  ') for x in valid[\"COE_NOTES\"]]\n",
    "#Removing Punctuations\n",
    "valid[\"COE_NOTES\"]  = valid[\"COE_NOTES\"] .str.replace('[^\\w\\s]','')\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "valid[\"COE_NOTES\"] = valid[\"COE_NOTES\"].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "#Rare Word Removal\n",
    "freq = pd.Series(' '.join(valid[\"COE_NOTES\"]).split()).value_counts()[-20:]\n",
    "#Rare Word Removal\n",
    "freq = pd.Series(' '.join(valid[\"COE_NOTES\"]).split()).value_counts()[-20:]\n",
    "\n",
    "freq = list(freq.index)\n",
    "valid[\"COE_NOTES\"] = valid[\"COE_NOTES\"].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "valid[\"COE_NOTES\"].head()\n",
    "\n",
    "#Lemmatization\n",
    "from textblob import Word\n",
    "valid[\"COE_NOTES\"] = valid[\"COE_NOTES\"].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "valid[\"COE_NOTES\"].head()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#valid[\"COE_NOTES\"] = valid[\"COE_NOTES\"].apply(lambda x: \" \".join(x for x in x.split() if x in unique_words))\n",
    "#valid[\"COE_NOTES\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = data[\"NOTES_COE\"]\n",
    "#y= data['Final_Category_Encoded']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X is 1D currently because it will be passed to Vectorizer to become a 2D matrix\n",
    "\n",
    "You must always have a 1D object so CountVectorizer can turn into a 2D object for the model to be built on\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,random_state=1,train_size = 0.8)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4554,)\n",
      "(1974,)\n",
      "(4554,)\n",
      "(1974,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train = data[\"NOTES_COE\"]\n",
    "X_test = valid[\"COE_NOTES\"]\n",
    "y_train = data[\"Final_Category_Encoded\"]\n",
    "y_test = valid[\"Final_Category_Encoded1\"]\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split then vectorize (correct way)\n",
    "\n",
    "We do the train/test split before the CountVectorizer to properly simulate the real world where our future data contains words we have not seen before\n",
    "\n",
    "After training your data and chose the best model, you would then train on all of your data before predicting actual future data to maximize learning.\n",
    "\n",
    "we are learning the vocabulary dictionary and it returns a Document-Term matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4554, 6412)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. import and instantiate CountVectorizer (with the default parameters)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# 2. instantiate CountVectorizer (vectorizer)\n",
    "vect = CountVectorizer(stop_words=None)\n",
    "\n",
    "# learn the 'vocabulary' of the training data (occurs in-place)\n",
    "X_train_dtm = vect.fit_transform(X_train.astype(str))\n",
    "X_train_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1974x6412 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 35284 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. transform testing data (using fitted vocabulary) into a document-term matrix\n",
    "X_test_dtm = vect.transform(X_test.astype(str))\n",
    "X_test_dtm\n",
    "#the number of columns, 4552, is the same as what we have learned above in X_train_dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF: Finally, we can even reduce the weightage of more common words like (the, is, an etc.) which occurs in all document. This is called as TF-IDF i.e Term Frequency times inverse document frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf = TfidfTransformer()\n",
    "X_train_tfidf = tfidf.fit_transform(X_train_dtm)\n",
    "#X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tfidf = tfidf.transform(X_test_dtm)\n",
    "#X_test_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_valid_tfidf = tfidf_transformer.transform(X_valid_dtm)\n",
    "#X_valid_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4554x6412 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 78758 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#from nltk.stem.porter import PorterStemmer\n",
    "#porter_stemmer = PorterStemmer()\n",
    "#X_train_dtm = X_train_dtm.apply(lambda x: ''.join([porter_stemmer.stem(X_train_dtm) for X_train_dtm in x.split()])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "ros = RandomOverSampler()\n",
    "X_train_balanced, y_train_balanced= ros.fit_resample(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unique, counts = np.unique(y_train_balanced, return_counts=True)\n",
    "#print(list(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test_balanced, y_test_balanced= ros.fit_sample(X_test_tfidf, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.29 s\n",
      "accuracy_score: 0.35916919959473154\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(penalty=\"l1\")\n",
    "%time lr.fit(X_train_balanced, y_train_balanced)\n",
    "#make class predictions for X_test_dtm\n",
    "y_pred_class = lr.predict(X_test_tfidf)\n",
    "# calculate accuracy of class predictions\n",
    "print(\"accuracy_score:\",accuracy_score(y_test, y_pred_class))\n",
    "# calculate AUC\n",
    "#print(\"f1_score:\",f1_score(y_test, y_pred_class,average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.92 s\n",
      "accuracy_score: 0.3009118541033435\n",
      "f1_score: 0.3247826307812787\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "lrsvc = LinearSVC()\n",
    "%time lrsvc.fit(X_train_dtm, y_train)\n",
    "#make class predictions for X_test_dtm\n",
    "y_pred_class = lrsvc.predict(X_test_tfidf)\n",
    "# calculate accuracy of class predictions\n",
    "print(\"accuracy_score:\",accuracy_score(y_test, y_pred_class))\n",
    "# calculate AUC\n",
    "print(\"f1_score:\",f1_score(y_test, y_pred_class,average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "SGDC = SGDClassifier()\n",
    "%time SGDC.fit(X_train_balanced, y_train_balanced)\n",
    "#make class predictions for X_test_dtm\n",
    "y_pred_class = SGDC.predict(X_test_tfidf)\n",
    "# calculate accuracy of class predictions\n",
    "print(\"accuracy_score:\",accuracy_score(y_test, y_pred_class))\n",
    "# calculate AUC\n",
    "print(\"f1_score:\",f1_score(y_test, y_pred_class,average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "SGDC = SGDClassifier()\n",
    "%time SGDC.fit(X_train_balanced,y_train_balanced)\n",
    "#make class predictions for X_test_dtm\n",
    "y_pred_class = SGDC.predict(X_test_tfidf)\n",
    "# calculate accuracy of class predictions\n",
    "print(\"accuracy_score:\",accuracy_score(y_test, y_pred_class))\n",
    "# calculate AUC\n",
    "print(\"f1_score:\",f1_score(y_test, y_pred_class,average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([('vect', CountVectorizer(stop_words='english')),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('lr', LogisticRegression()),\n",
    " ])\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import nltk\n",
    "nltk.download()\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: ([stemmer.stem(w) for w in analyzer(doc)])\n",
    "stemmed_count_vect = StemmedCountVectorizer(stop_words='english')\n",
    "text_mnb_stemmed = Pipeline([('vect', stemmed_count_vect),\n",
    "                    ('tfidf', TfidfTransformer()),\n",
    "                      ('mnb', MultinomialNB(fit_prior=False)),\n",
    "])\n",
    "text_mnb_stemmed = text_mnb_stemmed.fit(X_train_balanced,y_train_balanced)\n",
    "predicted_mnb_stemmed = text_mnb_stemmed.predict(X_test_dtm)\n",
    "np.mean(predicted_mnb_stemmed == twenty_test.target)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "X1 = data.NOTES_COE\n",
    "y1 = data.Final_Category_Encoded\n",
    "\n",
    "ss = ShuffleSplit(n_splits=10, test_size=0.2)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr1 = LogisticRegression()\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "rs = RandomOverSampler()\n",
    "\n",
    "accs = []\n",
    "f1s = []\n",
    "\n",
    "\n",
    "for train_index, test_index in ss.split(X):\n",
    "    \n",
    "    X_train, X_test = X1.iloc[train_index], X1.iloc[test_index]\n",
    "    y_train, y_test = y1.iloc[train_index], y1.iloc[test_index]\n",
    "    \n",
    "    # Fit vectorizer and transform X train, then transform X test\n",
    "    X_train_vect = vect.fit_transform(X_train.astype(str))\n",
    "    X_test_vect = vect.transform(X_test.astype(str))\n",
    "    \n",
    "    # tfidf\n",
    "    X_train_with_tfidf = tfidf.fit_transform(X_train_vect)\n",
    "    X_test_with_tfidf = tfidf.transform(X_test_vect)\n",
    "    \n",
    "    \n",
    "    # Oversample\n",
    "    X_train_res, y_train_res = rs.fit_sample(X_train_with_tfidf, y_train)\n",
    "    \n",
    "    # #Fitting and Predictions\n",
    "    \n",
    "    lr1.fit(X_train_res, y_train_res)\n",
    "    y_pred = lr1.predict(X_test_with_tfidf)\n",
    "    \n",
    "    # Determine test set accuracy and f1 score on this fold using the true y labels and predicted y labels\n",
    "    accs.append(accuracy_score(y_test, y_pred))\n",
    "    #f1s.append(f1_score(y_test, y_pred,average='macro'))\n",
    "\n",
    "    \n",
    "print(\"\\nAverage accuracy across folds: {:.2f}%\".format(sum(accs) / len(accs) * 100))\n",
    "print(\"\\nAverage F1 score across folds: {:.2f}%\".format(sum(f1s) / len(f1s) * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "X1 = data.NOTES_COE\n",
    "y1 = data.Final_Category_Encoded\n",
    "\n",
    "ss = ShuffleSplit(n_splits=10, test_size=0.2)\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "SGDC = SGDClassifier(penalty=\"elasticnet\")\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "rs = RandomOverSampler()\n",
    "\n",
    "accs = []\n",
    "f1s = []\n",
    "\n",
    "\n",
    "for train_index, test_index in ss.split(X):\n",
    "    \n",
    "    X_train, X_test = X1.iloc[train_index], X1.iloc[test_index]\n",
    "    y_train, y_test = y1.iloc[train_index], y1.iloc[test_index]\n",
    "    \n",
    "    # Fit vectorizer and transform X train, then transform X test\n",
    "    X_train_vect = vect.fit_transform(X_train.astype(str))\n",
    "    X_test_vect = vect.transform(X_test.astype(str))\n",
    "    \n",
    "    # tfidf\n",
    "    X_train_with_tfidf = tfidf.fit_transform(X_train_vect)\n",
    "    X_test_with_tfidf = tfidf.transform(X_test_vect)\n",
    "    \n",
    "    \n",
    "    # Oversample\n",
    "    X_train_res, y_train_res = rs.fit_sample(X_train_with_tfidf, y_train)\n",
    "    \n",
    "    # #Fitting and Predictions\n",
    "    \n",
    "    SGDC.fit(X_train_res, y_train_res)\n",
    "    y_pred = SGDC.predict(X_test_with_tfidf)\n",
    "    \n",
    "    # Determine test set accuracy and f1 score on this fold using the true y labels and predicted y labels\n",
    "    accs.append(accuracy_score(y_test, y_pred))\n",
    "    f1s.append(f1_score(y_test, y_pred,average='macro'))\n",
    "\n",
    "    \n",
    "print(\"\\nAverage accuracy across folds: {:.2f}%\".format(sum(accs) / len(accs) * 100))\n",
    "print(\"\\nAverage F1 score across folds: {:.2f}%\".format(sum(f1s) / len(f1s) * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "X1 = data.NOTES_COE\n",
    "y1 = data.Final_Category_Encoded\n",
    "\n",
    "ss = ShuffleSplit(n_splits=5, test_size=0.2)\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "lrsvc  = LinearSVC()\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "rs = RandomOverSampler()\n",
    "\n",
    "accs = []\n",
    "f1s = []\n",
    "\n",
    "\n",
    "for train_index, test_index in ss.split(X):\n",
    "    \n",
    "    X_train, X_test = X1.iloc[train_index], X1.iloc[test_index]\n",
    "    y_train, y_test = y1.iloc[train_index], y1.iloc[test_index]\n",
    "    \n",
    "    # Fit vectorizer and transform X train, then transform X test\n",
    "    X_train_vect = vect.fit_transform(X_train.astype(str))\n",
    "    X_test_vect = vect.transform(X_test.astype(str))\n",
    "    \n",
    "      # tfidf\n",
    "    X_train_with_tfidf = tfidf.fit_transform(X_train_vect)\n",
    "    X_test_with_tfidf = tfidf.transform(X_test_vect)\n",
    "    \n",
    "    \n",
    "    # Oversample\n",
    "    X_train_res, y_train_res = rs.fit_sample(X_train_with_tfidf, y_train)\n",
    "    \n",
    "    \n",
    "    lrsvc.fit(X_train_res, y_train_res)\n",
    "    y_pred = lrsvc.predict(X_test_with_tfidf)\n",
    "    \n",
    "    \n",
    "    # Determine test set accuracy and f1 score on this fold using the true y labels and predicted y labels\n",
    "    accs.append(accuracy_score(y_test, y_pred))\n",
    "    f1s.append(f1_score(y_test, y_pred,average='macro'))\n",
    "    \n",
    "    #predictions\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "print(\"\\nAverage accuracy across folds: {:.2f}%\".format(sum(accs) / len(accs) * 100))\n",
    "print(\"\\nAverage F1 score across folds: {:.2f}%\".format(sum(f1s) / len(f1s) * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = { \"penalty\":['l2','l1'],          \n",
    "              'C': [0.0001,.001,.01,.1,1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf = GridSearchCV(lr, parameters, n_jobs=-1,cv= ss)\n",
    "gs_clf = gs_clf.fit(X_train_vect, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = pd.read_excel(\"D://final_data//Data_W45_28_March_lang.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid[\"COE_NOTES\"] = lb.transform(valid[\"Reason_for_Cancellation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid[\"COE_NOTES\"] = valid[\"COE_NOTES\"].astype(str).str.upper()\n",
    "valid[\"COE_NOTES\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from nltk.stem import PorterStemmer\n",
    "port = PorterStemmer()\n",
    "valid['COE_NOTES']=valid['COE_NOTES'].apply(lambda x : filter(None,x(\" \")))\n",
    "valid['COE_NOTES']=valid['COE_NOTES'].apply(lambda x : [port.stem(y) for y in x])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stop words removal\n",
    "#from nltk.corpus import stopwords\n",
    "#stop = stopwords.words('english') +self_stop\n",
    "#valid['NOTE_TXT'] = valid['NOTE_TXT'].apply(lambda x: ' '.join([word for word in x() if word not in (stop)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid[\"COE_NOTES\"] = valid[\"COE_NOTES\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "valid[\"COE_NOTES\"] = valid[\"COE_NOTES\"].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rare words removal\n",
    "#freq2 = pd.Series(' '.join(valid['NOTE_TXT']).split()).value_counts()[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#freq2 = list(freq2.index)\n",
    "#valid['NOTE_TXT'] = valid['NOTE_TXT'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = valid[\"NOTE_TXT\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vect = vect.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_idf = tfidf.transform(X_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid[\"predict_cat\"] = lrsvc.predict(X_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid.to_excel(\"D://final_data//out1_now_1.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
