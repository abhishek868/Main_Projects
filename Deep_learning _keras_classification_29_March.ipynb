{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "import itertools\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "# print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "import itertools\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import pandas  as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB,BernoulliNB\n",
    "nb = MultinomialNB()\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder \n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.svm import SVC\n",
    "from scipy.sparse import hstack, vstack\n",
    "from nltk.corpus import stopwords  \n",
    "#nltk.download('stopwords') \n",
    "import pickle\n",
    "import datetime\n",
    "from dateutil import parser\n",
    "import sys\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import f1_score ,accuracy_score,confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"C://Users//Abhishek_Prasar//Documents//My Received Files//train_29_march_preprocessed.xlsx\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "lb = LabelEncoder()\n",
    "data['Final_Category_Encoded'] = lb.fit_transform(data['Reason_for_Cancellation_COE'])\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Preprocessing Training Set\n",
    "data[\"NOTES_COE\"] = data[\"NOTES_COE\"].astype(str)\n",
    "data[\"NOTES_COE\"] = data[\"NOTES_COE\"].str.upper()\n",
    "data[\"NOTES_COE\"] = [x.replace(':', ' ') for x in data[\"NOTES_COE\"]]\n",
    "#Removing Punctuations\n",
    "data[\"NOTES_COE\"]  = data[\"NOTES_COE\"] .str.replace('[^\\w\\s]','')\n",
    "#from nltk.corpus import stopwords\n",
    "#stop = stopwords.words('english')\n",
    "#data[\"NOTES_COE\"] = data[\"NOTES_COE\"].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "#Rare Word Removal\n",
    "freq = pd.Series(' '.join(data[\"NOTES_COE\"]).split()).value_counts()[-20:]\n",
    "\n",
    "freq = list(freq.index)\n",
    "data[\"NOTES_COE\"] = data[\"NOTES_COE\"].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "data[\"NOTES_COE\"]\n",
    "\n",
    "#Lemmatization\n",
    "from textblob import Word\n",
    "data[\"NOTES_COE\"] = data[\"NOTES_COE\"].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "data[\"NOTES_COE\"].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = pd.read_excel(\"C:\\\\Users\\\\Abhishek_Prasar\\\\Documents\\\\My Received Files\\\\test_29_march_preprocessed.xlsx\")\n",
    "valid[['COE_NOTES','Reason_for_Cancellation_ENT']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Final_Category_Encoded1'] = lb.fit_transform(data['Reason_for_Cancellation_COE'])\n",
    "valid['Final_Category_Encoded1'] = lb.transform(valid['Reason_for_Cancellation_ENT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid[\"NOTES_COE\"] = valid[\"NOTES_COE\"].str.upper()\n",
    "valid[\"NOTES_COE\"] = [x.replace(':', ' ') for x in valid[\"NOTES_COE\"]]\n",
    "#Removing Punctuations\n",
    "valid[\"NOTES_COE\"]  = valid[\"NOTES_COE\"] .str.replace('[^\\w\\s]','')\n",
    "#from nltk.corpus import stopwords\n",
    "#stop = stopwords.words('english')\n",
    "#valid[\"NOTES_COE\"] = valid[\"NOTES_COE\"].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "#Rare Word Removal\n",
    "freq = pd.Series(' '.join(valid[\"NOTES_COE\"]).split()).value_counts()[-20:]\n",
    "\n",
    "freq = list(freq.index)\n",
    "valid[\"NOTES_COE\"] = valid[\"NOTES_COE\"].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "valid[\"NOTES_COE\"].head()\n",
    "\n",
    "#Lemmatization\n",
    "from textblob import Word\n",
    "valid[\"NOTES_COE\"] = valid[\"NOTES_COE\"].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "valid[\"NOTES_COE\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from tensorflow import keras\n",
    "layers = keras.layers\n",
    "models = keras.models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data[\"NOTES_COE\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = valid[\"NOTES_COE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 1000\n",
    "tokenize = keras.preprocessing.text.Tokenizer(num_words=max_words,char_level=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize.fit_on_texts(X_train) # fit tokenizer to our training text data\n",
    "tokenize.fit_on_texts(X_test)\n",
    "x_train = tokenize.texts_to_matrix(X_train)\n",
    "x_test = tokenize.texts_to_matrix(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = data[\"Final_Category_Encoded1\"]\n",
    "y_test = valid[\"Final_Category_Encoded1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the labels to a one-hot representation\n",
    "num_classes = np.max(y_train) + 1\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the dimenstions of our training and test data (this is helpful to debug)\n",
    "print('X_train shape:', x_train.shape)\n",
    "print('X_test shape:', x_test.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: The data processed for this output had the max text length set to 400.\n",
    "# for batch_size in range(10,31,10):\n",
    "#   for epochs in range(3,15,5):\n",
    "#     for drop_ratio in np.linspace(0.1, 0.5, 3):\n",
    "#       run_experiment(batch_size, epochs, drop_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Traing Model\n",
    "batch_size = 50\n",
    "epochs = 8\n",
    "drop_ratio = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(512, input_shape=(max_words,)))\n",
    "model.add(layers.Activation('relu'))\n",
    "# model.add(layers.Dropout(drop_ratio))\n",
    "model.add(layers.Dense(num_classes))\n",
    "model.add(layers.Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The validation_split param tells Keras what % of our training data should be used in the validation set\n",
    "# You can see the validation loss decreasing slowly when you run this\n",
    "# Because val_loss is no longer decreasing we stop training to prevent overfitting\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the accuracy of our trained model\n",
    "score = model.evaluate(x_test, y_test,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(batch_size, epochs, drop_ratio):\n",
    "  print('batch size: {}, epochs: {}, drop_ratio: {}'.format(\n",
    "      batch_size, epochs, drop_ratio))\n",
    "  model = models.Sequential()\n",
    "  model.add(layers.Dense(512, input_shape=(max_words,)))\n",
    "  model.add(layers.Activation('sigmoid'))\n",
    "  model.add(layers.Dropout(drop_ratio))\n",
    "  model.add(layers.Dense(num_classes))\n",
    "  model.add(layers.Activation('softmax'))\n",
    "\n",
    "  model.compile(loss='categorical_crossentropy',\n",
    "                optimizer='adam',\n",
    "                metrics=['accuracy'])\n",
    "  history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=0,\n",
    "                    validation_split=0.1)\n",
    "  score = model.evaluate(x_test, y_test,\n",
    "                       batch_size=batch_size, verbose=0)\n",
    "  print('\\tTest loss:', score[0])\n",
    "  print('\\tTest accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 20\n",
    "drop_ratio = .5\n",
    "run_experiment(batch_size, epochs, drop_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_softmax = model.predict(x_test)\n",
    "\n",
    "y_test_1d = []\n",
    "y_pred_1d = []\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    probs = y_test[i]\n",
    "    index_arr = np.nonzero(probs)\n",
    "    one_hot_index = index_arr[0].item(0)\n",
    "    y_test_1d.append(one_hot_index)\n",
    "\n",
    "for i in range(0, len(y_softmax)):\n",
    "    probs = y_softmax[i]\n",
    "    predicted_index = np.argmax(probs)\n",
    "    y_pred_1d.append(predicted_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid[\"predicted\"]= y_pred_1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid.to_excel(\"D://final_data//output_with_deep_learning.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
